# Отчет 2. Исследование метода Q-learning в среде Frozen Lake 

## 1. Сравнение алгоритмов V и Q learning (2 балла)
Для алгоритма `V learning` на поле (4х4) при `gamma=0.9` сходимость (mean reward > 0.85) достигается в среднем за 53 итерации (от 30 до 85). 
Графики зависимости reward от количества итераций приведены ниже. 

![alt text](v9.png)

Для алгоритма `Q learning` на поле (4х4) при `gamma=0.9` сходимость (mean reward > 0.85) достигается в среднем за 53 итерации (от 30 до 85). 
Графики зависимости reward от количества итераций приведены ниже. 

![alt text](q9.png)

**Вывод:** Алгоритм обучения ценности состояний более эффективен чем обучение ценности действий. Это связано с тем, что ... 
 что алгоритм Q learning для каждоый последующей стратегии использует ранее накопленный опыт


## 2. Влияние гиперпараметра `GAMMA` на скорость сходимости . (2 балла)

Для алгоритма `Q learning` и  `V learning` на поле (4х4) при `gamma=0.7` сходимость (mean reward > 0.85) достигается в среднем за 53 итерации (от 30 до 85). 
Графики зависимости reward от количества итераций приведены ниже. 

![alt text](q7.png)
![alt text](v7.png)

**Вывод:** Увеличение гиперпараметра `GAMMA` приводит к увеличению сходимости (меньше эпизодов) и Это связано с тем, что при увеличении гамма увеличивается награда 

Уменьшение гиперпараметра `GAMMA` приводит к  Это связано с тем, что ... 

## 3. Сравнение алгоритмов V и Q learning на поле большего размера (3 балла)
