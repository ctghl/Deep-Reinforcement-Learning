# Отчет 2. Исследование метода Q-learning в среде Frozen Lake 

## 1. Сравнение алгоритмов V и Q learning (2 балла)
Для алгоритма `V learning` на поле (4х4) при `gamma=0.9` сходимость (mean reward > 0.85) достигается в среднем за  итерации (от 30 до 85). 
Графики зависимости reward от количества итераций приведены ниже. 
![alt text](image-4.png)


Для алгоритма `Q learning` на поле (4х4) при `gamma=0.9` сходимость (mean reward > 0.85) достигается в среднем за  итерации (от 30 до 85). 
Графики зависимости reward от количества итераций приведены ниже. 

![alt text](image-1.png)

**Вывод:** Алгоритм обучения ценности состояний более эффективен чем обучение ценности действий. Это связано с тем, что алгоритм Q learning для каждоый последующей стратегии использует ранее накопленный опыт


## 2. Влияние гиперпараметра `GAMMA` на скорость сходимости . (2 балла)

Для алгоритма `Q learning` и  `V learning` на поле (4х4) при `gamma=0.8` сходимость (mean reward > 0.85) достигается в среднем за 53 итерации (от 30 до 85). 
Графики зависимости reward от количества итераций приведены ниже. 
Q
![alt text](image.png)
V
![alt text](image-3.png)
**Вывод:** Увеличение гиперпараметра `GAMMA` приводит к увеличению сходимости (меньше эпизодов) и Это связано с тем, что при увеличении гамма увеличивается награда, снижается стоимость 1 действия

Уменьшение гиперпараметра `GAMMA` приводит к уменьшению сходимости  Это связано с тем, что, снижается стоимость 1 действия

## 3. Сравнение алгоритмов V и Q learning на поле большего размера (3 балла)
Q:
![alt text](image-2.png)
V:
![alt text](image-5.png)
Сходимость v learning быстрее для данного примера. 