# Отчет 3. Deep Q-learning 

## 1. Изучение алгоритма табличного обучения (tabular Q learning) в среде Frozen Lake (2 балла)
Для алгоритма `Deep Q learning` на поле (4х4) при `alpha=0.7` сходимость достигается в среднем за 12133 итераций (от 7274 до 16992). 
Графики зависимости reward от количества итераций приведены ниже. 
![alt text](image-4.png)

Для алгоритма `Deep Q learning` на поле (4х4) при `alpha=0.5` сходимость достигается в среднем за 14234 итерации (от 5219 до 23249). 
Графики зависимости reward от количества итераций приведены ниже. 

![alt text](image-3.png)
Для алгоритма `Deep Q learning` на поле (4х4) при `alpha=0.9` сходимость достигается в среднем за 11196 итераций (от 776 до 21161). 
Графики зависимости reward от количества итераций приведены ниже. 

![alt text](image-5.png)
**Вывод:** .


## 2. Изучение алгоритма глубокого обучения (Deep Q learning) в среде Pong. (2 балла)

Обучите сеть с гиперпараметрами по умолчанию и запишите видео.


ошибка со средой lib

## 3. Исследуйте влияние гиперпараметра (Deep Q learning в среде Pong, на Ваш выбор) на среднее количество шагов обучения. (3 балла)

При обычных параметрах модель обучилась за 1 430 059 шагов. Значительный прирост в скорости дало подключение GPU, 22 f/s против 88 f/s. Скорость увеличилась в 4 раза. (Параметры устройства - процессор Intel Core i5 11-ого поколения, видеокарта - GeForce RTX 3060)

Для исследования выбран параметр`gamma`: 0,5 и 0,3 - доля вознаграждений от суммарных. 
При увеличении параметра обучение проиходит дольше


