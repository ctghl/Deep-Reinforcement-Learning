# Отчет 3. Deep Q-learning 

## 1. Изучение алгоритма табличного обучения (tabular Q learning) в среде Frozen Lake (2 балла)
Для алгоритма `Deep Q learning` на поле (4х4) при `alpha=0.7` сходимость достигается в среднем за 12133 итераций (от 7274 до 16992). 
Графики зависимости reward от количества итераций приведены ниже. 
![alt text](image-4.png)

Для алгоритма `Deep Q learning` на поле (4х4) при `alpha=0.5` сходимость достигается в среднем за 9567 итерации (от 3700 до 14696). 
Графики зависимости reward от количества итераций приведены ниже. 

![alt text](image-3.png)
Для алгоритма `Deep Q learning` на поле (4х4) при `alpha=0.9` сходимость достигается в среднем за 12000 итераций (от 3776 до 20766). 
Графики зависимости reward от количества итераций приведены ниже. 

![alt text](image-5.png)


При изменении 'alpha' в меньшую сторону приводит к увеличению скорости сходимости, при увеличении - у уменьшению.
'alpha' отвечает за "вес" нового значения. Отлимальное значение - 0.5 - ориентируемся на предыдущие и последующие значения


## 2. Изучение алгоритма глубокого обучения (Deep Q learning) в среде Pong. (2 балла)

Обучите сеть с гиперпараметрами по умолчанию и запишите видео.


ошибка со средой lib

## 3. Исследуйте влияние гиперпараметра (Deep Q learning в среде Pong, на Ваш выбор) на среднее количество шагов обучения. (3 балла)

При обычных параметрах модель обучилась за 1 430 059 шагов. Значительный прирост в скорости дало подключение GPU, 22 f/s против 88 f/s. Скорость увеличилась в 4 раза. (Параметры устройства - процессор Intel Core i5 11-ого поколения, видеокарта - GeForce RTX 3060)

Для исследования выбран параметр`gamma`: 0,5 и 0,3 - доля вознаграждений от суммарных. 
При увеличении параметра обучение проиходит дольше


